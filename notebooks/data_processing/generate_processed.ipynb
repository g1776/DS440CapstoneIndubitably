{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate processed data\n",
    "Here we will combine the subset of reviews with their labels and associated property description to create a processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which GEO to process?\n",
    "GEO = \"texas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the relevant data\n",
    "DATA_FP = \"../../data\"\n",
    "LABELS = pd.read_csv(DATA_FP + f\"/labels/{GEO}_reviews_labels.csv\")\n",
    "SUBSET = pd.read_csv(DATA_FP + f\"/filtered/{GEO}_reviews_filtered.csv\")\n",
    "LISTINGS = pd.read_csv(DATA_FP + f\"/raw/{GEO}_listings.csv\", encoding=\"unicode_escape\", low_memory=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we are dropping labels of \"maybe\". These are reviews we weren't sure about the label of. If we need more training data, we can revisit these reviews for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of maybe/other reviews: 22/1000\n"
     ]
    }
   ],
   "source": [
    "maybes_others = LABELS[(LABELS.label == \"maybe\") | (LABELS.label == \"other\")]\n",
    "print(f\"Number of maybe/other reviews: {len(maybes_others)}/{len(LABELS)}\")\n",
    "\n",
    "LABELS = LABELS[~((LABELS.label == \"maybe\") | (LABELS.label == \"other\"))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with labels\n",
    "subset_with_labels = pd.merge(LABELS, SUBSET, on=\"id\", suffixes=(\"_labels\", \"_subset\"))\n",
    "\n",
    "# join with listings\n",
    "subset_labels_and_listing = pd.merge(subset_with_labels, LISTINGS, left_on=\"listing_id\", right_on=\"id\", suffixes=(\"\",\"_listings\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the columns we need for the processed dataset. Also, rename the columns to be more descriptive, and clean the amentities column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>amenities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>62538105</td>\n",
       "      <td>324552</td>\n",
       "      <td>This is a private detached studio apartmnet in...</td>\n",
       "      <td>This place gets 5 out of 5 stars for location....</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>no</td>\n",
       "      <td>South Congress private SOCO Studio</td>\n",
       "      <td>[Wifi, Air conditioning, Kitchen, Free parking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>69599634</td>\n",
       "      <td>83643</td>\n",
       "      <td>Join me in my comfy house. I'll entertain you ...</td>\n",
       "      <td>Michele made me feel very welcome on my arriva...</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>no</td>\n",
       "      <td>Fun Central Austin Convenience</td>\n",
       "      <td>[Internet, Wifi, Air conditioning, Kitchen, Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>430472854</td>\n",
       "      <td>915082</td>\n",
       "      <td>Modern 2 Bedroom 2 bath just 3 blocks from Sou...</td>\n",
       "      <td>The unit its self is very nice and up to date....</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>SoCo: Ultra-Modern 2 bed 2 bath privacy &amp; loca...</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>74237263</td>\n",
       "      <td>958172</td>\n",
       "      <td>Large studio in the back house.  Tucked away i...</td>\n",
       "      <td>Though I didn't meet Donna myself, she was ver...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>An ArtistÛªs  Downtown Paradise....</td>\n",
       "      <td>[Wifi, Air conditioning, Kitchen, Free street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>8712775</td>\n",
       "      <td>362662</td>\n",
       "      <td>Downtown Central Austin Best Location Huge New...</td>\n",
       "      <td>Robert's house is just awesome. Plenty of room...</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>no</td>\n",
       "      <td>Downtown Central Austin HUGE HOUSE!</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_id  listing_id                                        description  \\\n",
       "264   62538105      324552  This is a private detached studio apartmnet in...   \n",
       "123   69599634       83643  Join me in my comfy house. I'll entertain you ...   \n",
       "658  430472854      915082  Modern 2 Bedroom 2 bath just 3 blocks from Sou...   \n",
       "728   74237263      958172  Large studio in the back house.  Tucked away i...   \n",
       "351    8712775      362662  Downtown Central Austin Best Location Huge New...   \n",
       "\n",
       "                                              comments  sentiment label  \\\n",
       "264  This place gets 5 out of 5 stars for location....   3.750000    no   \n",
       "123  Michele made me feel very welcome on my arriva...   4.333333    no   \n",
       "658  The unit its self is very nice and up to date....   4.000000    no   \n",
       "728  Though I didn't meet Donna myself, she was ver...   4.000000    no   \n",
       "351  Robert's house is just awesome. Plenty of room...   4.500000    no   \n",
       "\n",
       "                                                  name  \\\n",
       "264                 South Congress private SOCO Studio   \n",
       "123                     Fun Central Austin Convenience   \n",
       "658  SoCo: Ultra-Modern 2 bed 2 bath privacy & loca...   \n",
       "728               An ArtistÛªs  Downtown Paradise....   \n",
       "351                Downtown Central Austin HUGE HOUSE!   \n",
       "\n",
       "                                             amenities  \n",
       "264  [Wifi, Air conditioning, Kitchen, Free parking...  \n",
       "123  [Internet, Wifi, Air conditioning, Kitchen, Fr...  \n",
       "658  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "728  [Wifi, Air conditioning, Kitchen, Free street ...  \n",
       "351  [TV, Cable TV, Internet, Wifi, Air conditionin...  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep =[\n",
    "    \"id\",\n",
    "    \"listing_id\",\n",
    "    \"description\",\n",
    "    \"comments\",\n",
    "    \"sentiment\",\n",
    "    \"label\",\n",
    "    \"name\",\n",
    "    \"amenities\"\n",
    "]\n",
    "\n",
    "subset_labels_and_listing = subset_labels_and_listing[cols_to_keep]\n",
    "\n",
    "# rename id to review_id, for clarity\n",
    "subset_labels_and_listing = subset_labels_and_listing.rename(columns={\"id\": \"review_id\"})\n",
    "\n",
    "def parse_amenities(amenities):\n",
    "  amenities = amenities.replace(\"{\", \"\").replace(\"]\", \"\").replace('\"', \"\")\n",
    "  return amenities.split(\",\")\n",
    "\n",
    "subset_labels_and_listing.amenities = subset_labels_and_listing.amenities.apply(parse_amenities)\n",
    "\n",
    "subset_labels_and_listing.sample(n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean the text columns to make them more useful for our model. We will do the following:\n",
    "\n",
    "- Convert to lowercase\n",
    "- remove some specific phrases that we know are not useful\n",
    "- Remove punctuation\n",
    "- Remove stop words\n",
    "- Lemmatize words\n",
    "\n",
    "We will use nltk to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing description...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing comments...\n",
      "Preprocessing name...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>amenities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>45714753</td>\n",
       "      <td>850518</td>\n",
       "      <td>[newly, redone, eastside, open, air, cottage, ...</td>\n",
       "      <td>[brian, mollie, place, warm, welcoming, comfor...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>[prime, east, side, location]</td>\n",
       "      <td>[TV, Wifi, Air conditioning, Kitchen, Free par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>237887938</td>\n",
       "      <td>294708</td>\n",
       "      <td>[spacious, townhouse, 2, king, bed, 1, queen, ...</td>\n",
       "      <td>[good, location, maybe, not, suitable, neighbo...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>[3bd, loft, soco, area, convenience]</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>30616482</td>\n",
       "      <td>925342</td>\n",
       "      <td>[perfect, location, ut, campus, central, explo...</td>\n",
       "      <td>[carlton, very, nice, available, quick, messag...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>[ut, football, sxsw, acl, f, 1, bass, hall]</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>234671770</td>\n",
       "      <td>971055</td>\n",
       "      <td>[grad, student, furnished, studio, apartment, ...</td>\n",
       "      <td>[alex, great, person, responsible, solving, po...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>mbad</td>\n",
       "      <td>[cozy, quiet, studioapt, super, close, ut]</td>\n",
       "      <td>[Internet, Wifi, Air conditioning, Kitchen, Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>11744491</td>\n",
       "      <td>910696</td>\n",
       "      <td>[best, place, acl, sxsw, formula, 1, holiday, ...</td>\n",
       "      <td>[coming, brooklyn, frankly, coming, anywhere, ...</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>no</td>\n",
       "      <td>[mid, century, modern, w, spacious, yard]</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_id  listing_id                                        description  \\\n",
       "616   45714753      850518  [newly, redone, eastside, open, air, cottage, ...   \n",
       "226  237887938      294708  [spacious, townhouse, 2, king, bed, 1, queen, ...   \n",
       "676   30616482      925342  [perfect, location, ut, campus, central, explo...   \n",
       "752  234671770      971055  [grad, student, furnished, studio, apartment, ...   \n",
       "647   11744491      910696  [best, place, acl, sxsw, formula, 1, holiday, ...   \n",
       "\n",
       "                                              comments  sentiment label  \\\n",
       "616  [brian, mollie, place, warm, welcoming, comfor...   4.000000    no   \n",
       "226  [good, location, maybe, not, suitable, neighbo...   3.000000    no   \n",
       "676  [carlton, very, nice, available, quick, messag...   4.000000    no   \n",
       "752  [alex, great, person, responsible, solving, po...   2.000000  mbad   \n",
       "647  [coming, brooklyn, frankly, coming, anywhere, ...   4.333333    no   \n",
       "\n",
       "                                            name  \\\n",
       "616                [prime, east, side, location]   \n",
       "226         [3bd, loft, soco, area, convenience]   \n",
       "676  [ut, football, sxsw, acl, f, 1, bass, hall]   \n",
       "752   [cozy, quiet, studioapt, super, close, ut]   \n",
       "647    [mid, century, modern, w, spacious, yard]   \n",
       "\n",
       "                                             amenities  \n",
       "616  [TV, Wifi, Air conditioning, Kitchen, Free par...  \n",
       "226  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "676  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "752  [Internet, Wifi, Air conditioning, Kitchen, Fr...  \n",
       "647  [TV, Cable TV, Internet, Wifi, Air conditionin...  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove negative words from stopwords\n",
    "negative_words = [\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"nor\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"doesnt\",\n",
    "    \"couldnt\",\n",
    "    \"shouldnt\",\n",
    "    \"wouldnt\",\n",
    "    \"cant\",\n",
    "    \"cannot\",\n",
    "    \"wont\",\n",
    "    \"isnt\",\n",
    "    \"arent\",\n",
    "    \"wasnt\",\n",
    "    \"werent\",\n",
    "    \"hasnt\",\n",
    "    \"havent\",\n",
    "    \"hadnt\",\n",
    "    \"dont\",\n",
    "    \"didnt\",\n",
    "    \"neednt\",\n",
    "    \"very\"\n",
    "]\n",
    "for w in negative_words:\n",
    "    try:\n",
    "        stop_words.remove(w)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "additional_stopwords = [\n",
    "    \"airbnb\",\n",
    "    \"austin\",\n",
    "    \"texas\",\n",
    "    \"home\",\n",
    "    \"house\"\n",
    "]\n",
    "for w in additional_stopwords:\n",
    "    stop_words.add(w)\n",
    "\n",
    "# remove some specific phrases, using regular expressions\n",
    "specific_phrases = [\n",
    "    r\"\\(.* hidden by airbnb\\)\",\n",
    "]\n",
    "\n",
    "\n",
    "# download lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(tokens) -> list:\n",
    "    processed_tokens = []\n",
    "    for w in tokens:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        lemmatized = lemmatizer.lemmatize(w)\n",
    "        processed_tokens.append(lemmatized)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    # lowercase\n",
    "    text: str = text.lower()\n",
    "\n",
    "    for phrase in specific_phrases:\n",
    "        text = re.sub(phrase, \"\", text)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # remove stopwords and lemmatize\n",
    "    return remove_stopwords_and_lemmatize(tokens)\n",
    "\n",
    "print(\"Preprocessing description...\")\n",
    "subset_labels_and_listing.description = subset_labels_and_listing.description.apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing comments...\")\n",
    "subset_labels_and_listing.comments = subset_labels_and_listing.comments.apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing name...\")\n",
    "subset_labels_and_listing.name = subset_labels_and_listing.name.apply(preprocess_text)\n",
    "\n",
    "subset_labels_and_listing.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it\n",
    "subset_labels_and_listing.to_csv(DATA_FP + f\"/processed/{GEO}_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf086def782804284d24881115c612afdcc8ea791299ba67855f0c7f1a9ccc5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
