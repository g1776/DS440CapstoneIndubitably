{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate processed data\n",
    "Here we will combine the subset of reviews with their labels and associated property description to create a processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which GEO to process?\n",
    "GEO = \"texas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the relevant data\n",
    "DATA_FP = \"../../data\"\n",
    "LABELS = pd.read_csv(DATA_FP + f\"/labels/{GEO}_reviews_labels.csv\")\n",
    "SUBSET = pd.read_csv(DATA_FP + f\"/filtered/{GEO}_reviews_filtered.csv\")\n",
    "LISTINGS = pd.read_csv(DATA_FP + f\"/raw/{GEO}_listings.csv\", encoding=\"unicode_escape\", low_memory=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we are dropping labels of \"maybe\". These are reviews we weren't sure about the label of. If we need more training data, we can revisit these reviews for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of maybe reviews: 16/870\n"
     ]
    }
   ],
   "source": [
    "maybes = LABELS[LABELS.label == \"maybe\"]\n",
    "print(f\"Number of maybe reviews: {len(maybes)}/{len(LABELS)}\")\n",
    "\n",
    "LABELS = LABELS[LABELS.label != \"maybe\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with labels\n",
    "subset_with_labels = pd.merge(LABELS, SUBSET, on=\"id\", suffixes=(\"_labels\", \"_subset\"))\n",
    "\n",
    "# join with listings\n",
    "subset_labels_and_listing = pd.merge(subset_with_labels, LISTINGS, left_on=\"listing_id\", right_on=\"id\", suffixes=(\"\",\"_listings\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the columns we need for the processed dataset. Also, rename the columns to be more descriptive, and clean the amentities column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>amenities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>17510255</td>\n",
       "      <td>294708</td>\n",
       "      <td>Spacious townhouse. 2 king beds, 1 queen bed, ...</td>\n",
       "      <td>We found the condo very spacious as described....</td>\n",
       "      <td>3.5</td>\n",
       "      <td>no</td>\n",
       "      <td>3bd+Loft...SOCO Area Convenience</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>191735468</td>\n",
       "      <td>349447</td>\n",
       "      <td>Welcome to the very hip, unique and beautiful ...</td>\n",
       "      <td>Nice small studio home tucked in a quiet worki...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>no</td>\n",
       "      <td>Eastside Cabana - Near UT,Downtown</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>255194396</td>\n",
       "      <td>958172</td>\n",
       "      <td>Large studio in the back house.  Tucked away i...</td>\n",
       "      <td>Great place to stay! Space is beautiful with g...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>An ArtistÛªs  Downtown Paradise....</td>\n",
       "      <td>[Wifi, Air conditioning, Kitchen, Free street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>88445269</td>\n",
       "      <td>949054</td>\n",
       "      <td>The condo is in a superior location within wal...</td>\n",
       "      <td>Overall this rental was satisfactory.  The pic...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>yes</td>\n",
       "      <td>ª´of Downtown w/free parking! Sleeps 8 in 7 b...</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>157118137</td>\n",
       "      <td>217637</td>\n",
       "      <td>Take advantage of our Central East location to...</td>\n",
       "      <td>Felt like your own home. House has all the ess...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>East Austin Charmer -- Super Cozy</td>\n",
       "      <td>[TV, Internet, Wifi, Air conditioning, Kitchen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_id  listing_id                                        description  \\\n",
       "230   17510255      294708  Spacious townhouse. 2 king beds, 1 queen bed, ...   \n",
       "336  191735468      349447  Welcome to the very hip, unique and beautiful ...   \n",
       "752  255194396      958172  Large studio in the back house.  Tucked away i...   \n",
       "721   88445269      949054  The condo is in a superior location within wal...   \n",
       "185  157118137      217637  Take advantage of our Central East location to...   \n",
       "\n",
       "                                              comments  sentiment label  \\\n",
       "230  We found the condo very spacious as described....        3.5    no   \n",
       "336  Nice small studio home tucked in a quiet worki...        4.5    no   \n",
       "752  Great place to stay! Space is beautiful with g...        4.0    no   \n",
       "721  Overall this rental was satisfactory.  The pic...        2.5   yes   \n",
       "185  Felt like your own home. House has all the ess...        4.0    no   \n",
       "\n",
       "                                                  name  \\\n",
       "230                   3bd+Loft...SOCO Area Convenience   \n",
       "336                 Eastside Cabana - Near UT,Downtown   \n",
       "752               An ArtistÛªs  Downtown Paradise....   \n",
       "721  ª´of Downtown w/free parking! Sleeps 8 in 7 b...   \n",
       "185                  East Austin Charmer -- Super Cozy   \n",
       "\n",
       "                                             amenities  \n",
       "230  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "336  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "752  [Wifi, Air conditioning, Kitchen, Free street ...  \n",
       "721  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "185  [TV, Internet, Wifi, Air conditioning, Kitchen...  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep =[\n",
    "    \"id\",\n",
    "    \"listing_id\",\n",
    "    \"description\",\n",
    "    \"comments\",\n",
    "    \"sentiment\",\n",
    "    \"label\",\n",
    "    \"name\",\n",
    "    \"amenities\"\n",
    "]\n",
    "\n",
    "subset_labels_and_listing = subset_labels_and_listing[cols_to_keep]\n",
    "\n",
    "# rename id to review_id, for clarity\n",
    "subset_labels_and_listing = subset_labels_and_listing.rename(columns={\"id\": \"review_id\"})\n",
    "\n",
    "def parse_amenities(amenities):\n",
    "  amenities = amenities.replace(\"{\", \"\").replace(\"]\", \"\").replace('\"', \"\")\n",
    "  return amenities.split(\",\")\n",
    "\n",
    "subset_labels_and_listing.amenities = subset_labels_and_listing.amenities.apply(parse_amenities)\n",
    "\n",
    "subset_labels_and_listing.sample(n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean the text columns to make them more useful for our model. We will do the following:\n",
    "\n",
    "- Convert to lowercase\n",
    "- Remove punctuation\n",
    "- Remove stop words\n",
    "- Lemmatize words\n",
    "\n",
    "We will use nltk to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing description...\n",
      "Preprocessing comments...\n",
      "Preprocessing name...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>amenities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>102598692</td>\n",
       "      <td>363040</td>\n",
       "      <td>[opened, october, 2012, nicely, furnished, eff...</td>\n",
       "      <td>[long, time, austinite, moved, away, one, favo...</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>no</td>\n",
       "      <td>[zilker, festival, suite]</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>144798596</td>\n",
       "      <td>698082</td>\n",
       "      <td>[1940s, bungalow, quiet, street, thriving, cen...</td>\n",
       "      <td>[ellen, place, everything, hoped, great, locat...</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>no</td>\n",
       "      <td>[soco, bungalow, bombshell, ûócharming, cozy, ...</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>10037826</td>\n",
       "      <td>155359</td>\n",
       "      <td>[available, min, 90, day, overlooking, lake, t...</td>\n",
       "      <td>[stayed, weekend, great, experience, keith, go...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>[thetramhouse, 90, day, min]</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>33766112</td>\n",
       "      <td>585041</td>\n",
       "      <td>[lovely, historic, highly, desirable, location...</td>\n",
       "      <td>[absolutely, perfect, sister, bachelorette, pa...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>[soco, 6, bedroom, travis, height]</td>\n",
       "      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>69449529</td>\n",
       "      <td>632659</td>\n",
       "      <td>[mid, century, modern, conveniently, located, ...</td>\n",
       "      <td>[selfishly, speaking, want, write, review, kee...</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>no</td>\n",
       "      <td>[central, east, modern, guest]</td>\n",
       "      <td>[TV, Cable TV, Wifi, Air conditioning, Kitchen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     review_id  listing_id                                        description  \\\n",
       "367  102598692      363040  [opened, october, 2012, nicely, furnished, eff...   \n",
       "518  144798596      698082  [1940s, bungalow, quiet, street, thriving, cen...   \n",
       "159   10037826      155359  [available, min, 90, day, overlooking, lake, t...   \n",
       "444   33766112      585041  [lovely, historic, highly, desirable, location...   \n",
       "461   69449529      632659  [mid, century, modern, conveniently, located, ...   \n",
       "\n",
       "                                              comments  sentiment label  \\\n",
       "367  [long, time, austinite, moved, away, one, favo...   4.666667    no   \n",
       "518  [ellen, place, everything, hoped, great, locat...   4.500000    no   \n",
       "159  [stayed, weekend, great, experience, keith, go...   4.000000    no   \n",
       "444  [absolutely, perfect, sister, bachelorette, pa...   3.000000    no   \n",
       "461  [selfishly, speaking, want, write, review, kee...   4.500000    no   \n",
       "\n",
       "                                                  name  \\\n",
       "367                          [zilker, festival, suite]   \n",
       "518  [soco, bungalow, bombshell, ûócharming, cozy, ...   \n",
       "159                       [thetramhouse, 90, day, min]   \n",
       "444                 [soco, 6, bedroom, travis, height]   \n",
       "461                     [central, east, modern, guest]   \n",
       "\n",
       "                                             amenities  \n",
       "367  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "518  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "159  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "444  [TV, Cable TV, Internet, Wifi, Air conditionin...  \n",
       "461  [TV, Cable TV, Wifi, Air conditioning, Kitchen...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove negative words from stopwords\n",
    "negative_words = [\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"nor\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"doesnt\",\n",
    "    \"couldnt\",\n",
    "    \"shouldnt\",\n",
    "    \"wouldnt\",\n",
    "    \"cant\",\n",
    "    \"cannot\",\n",
    "    \"wont\",\n",
    "    \"isnt\",\n",
    "    \"arent\",\n",
    "    \"wasnt\",\n",
    "    \"werent\",\n",
    "    \"hasnt\",\n",
    "    \"havent\",\n",
    "    \"hadnt\",\n",
    "    \"dont\",\n",
    "    \"didnt\",\n",
    "    \"neednt\",\n",
    "    \"very\"\n",
    "]\n",
    "for w in negative_words:\n",
    "    try:\n",
    "        stop_words.remove(w)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "additional_stopwords = [\n",
    "    \"airbnb\",\n",
    "    \"austin\",\n",
    "    \"texas\",\n",
    "    \"home\",\n",
    "    \"house\"\n",
    "]\n",
    "for w in additional_stopwords:\n",
    "    stop_words.add(w)\n",
    "\n",
    "\n",
    "# download lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(tokens) -> list:\n",
    "    processed_tokens = []\n",
    "    for w in tokens:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        lemmatized = lemmatizer.lemmatize(w)\n",
    "        processed_tokens.append(lemmatized)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    # lowercase\n",
    "    text: str = text.lower()\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # remove stopwords and lemmatize\n",
    "    return remove_stopwords_and_lemmatize(tokens)\n",
    "\n",
    "print(\"Preprocessing description...\")\n",
    "subset_labels_and_listing.description = subset_labels_and_listing.description.apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing comments...\")\n",
    "subset_labels_and_listing.comments = subset_labels_and_listing.comments.apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing name...\")\n",
    "subset_labels_and_listing.name = subset_labels_and_listing.name.apply(preprocess_text)\n",
    "\n",
    "subset_labels_and_listing.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it\n",
    "subset_labels_and_listing.to_csv(DATA_FP + f\"/processed/{GEO}_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf086def782804284d24881115c612afdcc8ea791299ba67855f0c7f1a9ccc5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
