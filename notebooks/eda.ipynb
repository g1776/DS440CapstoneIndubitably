{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "In this notebook we take a very crude approach to filtering down the data to what may potentially be reviews that indicate misleading listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3156\\1823239506.py:1: DtypeWarning: Columns (43,61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  florida_l = pd.read_csv(\"../data/raw/florida_listings.csv\", encoding=\"unicode_escape\")\n",
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3156\\1823239506.py:2: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  texas_l = pd.read_csv(\"../data/raw/texas_listings.csv\", encoding=\"unicode_escape\")\n"
     ]
    }
   ],
   "source": [
    "florida_l = pd.read_csv(\"../data/raw/florida_listings.csv\", encoding=\"unicode_escape\")\n",
    "texas_l = pd.read_csv(\"../data/raw/texas_listings.csv\", encoding=\"unicode_escape\")\n",
    "florida_r = pd.read_csv(\"../data/raw/florida_reviews.csv\")\n",
    "texas_r = pd.read_csv(\"../data/raw/texas_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_amenities(amenities):\n",
    "  amenities = amenities.replace(\"{\", \"\").replace(\"]\", \"\").replace('\"', \"\")\n",
    "  return amenities.split(\",\")\n",
    "\n",
    "florida = pd.merge(florida_l, florida_r, left_on=\"id\", right_on=\"listing_id\", suffixes=(\"_listing\", \"_review\"))\n",
    "texas = pd.merge(texas_l, texas_r, left_on=\"id\", right_on=\"listing_id\", suffixes=(\"_listing\", \"_review\"))\n",
    "florida.amenities = florida.amenities.apply(parse_amenities)\n",
    "texas.amenities = texas.amenities.apply(parse_amenities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove negative words from stopwords\n",
    "negative_words = [\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"nor\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"doesnt\",\n",
    "    \"couldnt\",\n",
    "    \"shouldnt\",\n",
    "    \"wouldnt\",\n",
    "    \"cant\",\n",
    "    \"cannot\",\n",
    "    \"wont\",\n",
    "    \"isnt\",\n",
    "    \"arent\",\n",
    "    \"wasnt\",\n",
    "    \"werent\",\n",
    "    \"hasnt\",\n",
    "    \"havent\",\n",
    "    \"hadnt\",\n",
    "    \"dont\",\n",
    "    \"didnt\",\n",
    "    \"neednt\",\n",
    "    \"very\"\n",
    "]\n",
    "for w in negative_words:\n",
    "    try:\n",
    "        stop_words.remove(w)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "additional_stopwords = [\n",
    "    \"airbnb\",\n",
    "    \"austin\",\n",
    "    \"texas\",\n",
    "    \"home\",\n",
    "    \"house\"\n",
    "]\n",
    "for w in additional_stopwords:\n",
    "    stop_words.add(w)\n",
    "\n",
    "# remove some specific phrases, using regular expressions\n",
    "specific_phrases = [\n",
    "    r\"\\(.* hidden by airbnb\\)\",\n",
    "]\n",
    "\n",
    "\n",
    "# download lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(tokens) -> list:\n",
    "    processed_tokens = []\n",
    "    for w in tokens:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        lemmatized = lemmatizer.lemmatize(w)\n",
    "        processed_tokens.append(lemmatized)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "def preprocess_text(row, col) -> list:\n",
    "\n",
    "    if not isinstance(row[col], str):\n",
    "        return []\n",
    "\n",
    "    # lowercase\n",
    "    text: str = row[col].lower()\n",
    "\n",
    "    for phrase in specific_phrases:\n",
    "        text = re.sub(phrase, \"\", text)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # remove stopwords and lemmatize\n",
    "    return remove_stopwords_and_lemmatize(tokens)\n",
    "\n",
    "def preprocess_row(row):\n",
    "    print(row.name)\n",
    "    return {\n",
    "        \"description\": preprocess_text(row, \"description\"),\n",
    "        \"comments\": preprocess_text(row, \"comments\"),\n",
    "        \"amenities\": row[\"amenities\"],\n",
    "        \"listing_id\": row[\"id_listing\"],\n",
    "        \"review_id\": row[\"id_review\"],\n",
    "    }\n",
    "\n",
    "def preprocess_pipeline(df):\n",
    "    processed = df.apply(preprocess_row, axis=1)\n",
    "\n",
    "    # turn list of dicts into dataframe\n",
    "    to_return = pd.DataFrame()\n",
    "    for col in processed[0].keys():\n",
    "        to_return[col] = processed.apply(lambda x: x[col])\n",
    "\n",
    "    return to_return\n",
    "\n",
    "PROCESS = False\n",
    "\n",
    "if PROCESS:\n",
    "\n",
    "    florida_processed = preprocess_pipeline(florida)\n",
    "    texas_processed = preprocess_pipeline(texas)\n",
    "\n",
    "    florida_processed.to_csv(\"../data/raw/florida_raw_processed.csv\")\n",
    "    texas_processed.to_csv(\"../data/raw/texas_raw_processed.csv\")\n",
    "\n",
    "else:\n",
    "    florida_processed = pd.read_csv(\"../data/raw/florida_raw_processed.csv\")\n",
    "    texas_processed = pd.read_csv(\"../data/raw/texas_raw_processed.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data. First looking for reviews that mention amenities, then for with misleading and negation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = [\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"nor\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"doesnt\",\n",
    "    \"couldnt\",\n",
    "    \"shouldnt\",\n",
    "    \"wouldnt\",\n",
    "    \"cant\",\n",
    "    \"cannot\",\n",
    "    \"wont\",\n",
    "    \"isnt\",\n",
    "    \"arent\",\n",
    "    \"wasnt\",\n",
    "    \"werent\",\n",
    "    \"hasnt\",\n",
    "    \"havent\",\n",
    "    \"hadnt\",\n",
    "    \"dont\",\n",
    "    \"didnt\",\n",
    "    \"neednt\",\n",
    "    \"very\"\n",
    "]\n",
    "keywords = [\n",
    "        'deceiving', \n",
    "        \"decieving\", # I bet there's typos\n",
    "        'disappointing', \n",
    "        'horrible',\n",
    "        'terrible',\n",
    "        'awful',\n",
    "        'bad',\n",
    "        \"misleading\",\n",
    "        \"inaccurate\",\n",
    "        \"incorrect\",\n",
    "        \"missing\",\n",
    "        \"not as described\",\n",
    "        \"not there\",\n",
    "        \"wrong\",\n",
    "        \"not as pictured\",\n",
    "        \"lied\",\n",
    "        \"lie\",\n",
    "        \"liar\",\n",
    "        \"lying\",\n",
    "        \"fraud\",\n",
    "        \"fraudulent\",\n",
    "        \"scam\",\n",
    "        \"scammer\",\n",
    "        \"scamming\",\n",
    "        \"scammed\",\n",
    "        \"unsatisfactory\",\n",
    "        \"unacceptable\",\n",
    "        \"wasn't there\",\n",
    "        \"was not there\",\n",
    "        \"wasnt there\"\n",
    "        \"photoshopped\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "florida_processed.comments = florida_processed.comments.apply(lambda x: set(eval(x)))\n",
    "florida_processed.amenities = florida_processed.amenities.apply(lambda x: set(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for texas\n",
    "texas_processed.comments = texas_processed.comments.apply(lambda x: set(eval(x)))\n",
    "texas_processed.amenities = texas_processed.amenities.apply(lambda x: set(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_comments_with(row, col):\n",
    "    to_compare = set([x.lower() for x in row[col]])\n",
    "    return len(row.comments.intersection(to_compare)) > 0\n",
    "\n",
    "florida_with_amenity = florida_processed.comments[florida_processed.apply(lambda row: compare_comments_with(row, \"amenities\"), axis=1)]\n",
    "texas_with_amenity = texas_processed.comments[texas_processed.apply(lambda row: compare_comments_with(row, \"amenities\"), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_working = texas_processed.iloc[texas_with_amenity.index]\n",
    "fl_working = florida_processed.iloc[florida_with_amenity.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the comments that contain a negative word or misleading keyword\n",
    "negative_florida = fl_working[fl_working.comments.apply(lambda x: any([word in x for word in negative_words]))]\n",
    "negative_texas = tx_working[tx_working.comments.apply(lambda x: any([word in x for word in negative_words]))]\n",
    "misleading_florida = fl_working[fl_working.comments.apply(lambda x: any([word in x for word in keywords]))]\n",
    "misleading_texas = tx_working[tx_working.comments.apply(lambda x: any([word in x for word in keywords]))]\n",
    "\n",
    "# combine negative and misleading dataframes, keeping only the intersection\n",
    "neg_and_mis_florida = pd.merge(negative_florida, misleading_florida, how=\"inner\", left_index=True, right_index=True)\n",
    "neg_and_mis_texas = pd.merge(negative_texas, misleading_texas, how=\"inner\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of reviews with negative and/or misleading words, that also mention an amenity (florida):  11.230642764874373\n",
      "% of reviews with negative and/or misleading words, that also mention an amenity (texas):  6.594137874964619\n"
     ]
    }
   ],
   "source": [
    "print(\"% of reviews with negative and misleading words, that also mention an amenity (florida): \", len(neg_and_mis_florida)/len(florida) * 100)\n",
    "print(\"% of reviews with negative and misleading words, that also mention an amenity (texas): \", len(neg_and_mis_texas)/len(texas) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf086def782804284d24881115c612afdcc8ea791299ba67855f0c7f1a9ccc5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
